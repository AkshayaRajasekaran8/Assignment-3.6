1. 7TB is the available disk space per node (9 disks with 1 TB, 2 disk for operating system etc.
were excluded.). Assuming initial data size is 600 TB. How will you estimate the number of data
nodes (n)?


The formula for calculating number of data nodes is:
 
  n= H/d = c*r*S/(1-i)*d 

  H=c*r*s   and d=(1-i)*d

  where

  -H=Hadoop storage 

  -c = average compression ratio
   Compression ratio refers to the type of compression being used. If not  take c=1;

  -r=Replication factor
     Replication factor is usually 3 in the production cluster.

  -s=size of  data which is needed to be moved to hadoop
         It can be the combination of historical data and incremental data.
         The incremental data can be projected over a period of time (Take for example 3years) 

  -i=Intermediate factor
     Intermediate factor is usually  1/3 or 1/4

  -d= availability  of disk space per node


  The solution is:
  H=600TB   d=7TB

  n=H/d
    =600/7
    =85.7
 
  therfore, 85 data nodes are needed.


 
2. Imagine that you are uploading a file of 500MB into HDFS.100MB of data is successfully
uploaded into HDFS and another client wants to read the uploaded data while the upload is still in
progress. What will happen in such a scenario, will the 100 MB of data that is uploaded will it be
displayed?

* The default block size in Hadoop 1x is 64 MB and 128 MB in 2x . 
   Consider the block size to be 100 MB . Then we will have 5 blocks to be replicated thrice . 

 * We have 5 blocks for a file , client , name node and data node . 
   First the client will take the first block and then approach the name node for data node location to store this block . 

 * When the client is aware of  the datanode information it will see to the data node and start copying the first block . 
   Now, the client will get confirmation on the first block and it will initiate the same process for the second block . 

 * So, the reader will read the 100 MB uploaded data while the remaining upload is still in progress .  